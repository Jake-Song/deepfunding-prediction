{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[[ 0.61189759  1.8098      0.62177342  1.78638937]\n",
      "  [-0.34651609  0.45426068 -0.56022915  3.12183445]\n",
      "  [ 1.58851119  4.5799986   0.16529857  1.66663855]\n",
      "  [-0.48136681  5.22610408  0.34273198 -0.4900995 ]]\n",
      "\n",
      " [[ 0.61091015  3.04353009  1.78134331  3.122     ]\n",
      "  [ 1.77441617  4.54160191  1.72808821  4.58      ]\n",
      "  [ 3.07901913  6.2         3.10644938  6.15904689]\n",
      "  [ 0.88648166  7.99999496  1.97185305  4.23876558]]\n",
      "\n",
      " [[-0.52016824 -0.81555943  0.75116503  4.56975313]\n",
      "  [ 1.93177964  1.48844227  1.62459942  6.19999955]\n",
      "  [ 4.55920401  7.93418707  4.57367764  8.        ]\n",
      "  [ 6.1946142  10.          6.18545109  7.96659612]]\n",
      "\n",
      " [[-0.52804572 -0.4900995  -0.4900995   1.12486757]\n",
      "  [ 0.27889931  0.57910859 -0.25154735  6.63345855]\n",
      "  [ 2.479913    2.99720348  1.54721521  9.99373421]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Grid dimensions\n",
    "grid_size = 4\n",
    "\n",
    "# Define actions: 0=up, 1=down, 2=left, 3=right\n",
    "actions = [0, 1, 2, 3]\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1   # Learning rate\n",
    "gamma = 0.9   # Discount factor\n",
    "epsilon = 0.2 # Exploration rate\n",
    "num_episodes = 1000\n",
    "\n",
    "# Initialize Q-table with zeros: dimensions [rows, cols, actions]\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "def choose_action(state):\n",
    "    \"\"\"Choose an action using the epsilon-greedy policy.\"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        row, col = state\n",
    "        return np.argmax(Q[row, col])\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Take an action and return the new state, reward, and whether the episode is done.\"\"\"\n",
    "    row, col = state\n",
    "    # Determine new state based on action\n",
    "    if action == 0:   # up\n",
    "        new_row = max(row - 1, 0)\n",
    "        new_col = col\n",
    "    elif action == 1: # down\n",
    "        new_row = min(row + 1, grid_size - 1)\n",
    "        new_col = col\n",
    "    elif action == 2: # left\n",
    "        new_row = row\n",
    "        new_col = max(col - 1, 0)\n",
    "    elif action == 3: # right\n",
    "        new_row = row\n",
    "        new_col = min(col + 1, grid_size - 1)\n",
    "    \n",
    "    new_state = (new_row, new_col)\n",
    "    \n",
    "    # Check if the agent has reached the goal\n",
    "    if new_state == (grid_size - 1, grid_size - 1):\n",
    "        reward = 10\n",
    "        done = True\n",
    "    else:\n",
    "        reward = -1\n",
    "        done = False\n",
    "    return new_state, reward, done\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Start at the top-left corner\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        new_state, reward, done = step(state, action)\n",
    "        \n",
    "        row, col = state\n",
    "        new_row, new_col = new_state\n",
    "        best_next_action = np.argmax(Q[new_row, new_col])\n",
    "        \n",
    "        # Q-learning update rule\n",
    "        Q[row, col, action] = Q[row, col, action] + alpha * (\n",
    "            reward + gamma * Q[new_row, new_col, best_next_action] - Q[row, col, action]\n",
    "        )\n",
    "        \n",
    "        state = new_state\n",
    "\n",
    "print(\"Trained Q-table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (fastest) path to the goal:\n",
      "[(0, 0), (1, 0), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_best_path(Q):\n",
    "    state = (0, 0)\n",
    "    path = [state]\n",
    "    # A safeguard to avoid infinite loops (if something goes wrong)\n",
    "    max_steps = grid_size * grid_size\n",
    "    steps = 0\n",
    "    while state != (grid_size - 1, grid_size - 1) and steps < max_steps:\n",
    "        row, col = state\n",
    "        # Choose the best action according to the learned Q-values\n",
    "        action = np.argmax(Q[row, col])\n",
    "        state, reward, done = step(state, action)\n",
    "        path.append(state)\n",
    "        steps += 1\n",
    "    return path\n",
    "\n",
    "# After training, retrieve and print the optimal path\n",
    "best_path = get_best_path(Q)\n",
    "print(\"Best (fastest) path to the goal:\")\n",
    "print(best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
