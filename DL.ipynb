{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "part_path = Path('part-1')\n",
    "processed_path = Path(f'{part_path}/processed')\n",
    "\n",
    "df_train = pd.read_csv(f\"{part_path}/train.csv\")\n",
    "df_test = pd.read_csv(f\"{part_path}/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"{processed_path}/train-embeddings-only.csv\")\n",
    "df_test = pd.read_csv(f\"{processed_path}/test-embeddings-only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "    \n",
    "def process_embeddings(embedding_series):\n",
    "    return np.array([literal_eval(emb) for emb in embedding_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_a = process_embeddings(df_train.embedding)\n",
    "embeddings_b = process_embeddings(df_train.embedding_b)\n",
    "train_features = np.hstack([\n",
    "    embeddings_a,\n",
    "    embeddings_b\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_a = process_embeddings(df_test.embedding)\n",
    "embeddings_b = process_embeddings(df_test.embedding_b)\n",
    "test_features = np.hstack([\n",
    "    embeddings_a,\n",
    "    embeddings_b\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features\n",
    "y = df_train[\"weight_a\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.0225, Val Loss: 0.0219\n",
      "Epoch [10/100], Train Loss: 0.0144, Val Loss: 0.0204\n",
      "Epoch [15/100], Train Loss: 0.0113, Val Loss: 0.0165\n",
      "Epoch [20/100], Train Loss: 0.0110, Val Loss: 0.0140\n",
      "Epoch [25/100], Train Loss: 0.0096, Val Loss: 0.0154\n",
      "Epoch [30/100], Train Loss: 0.0081, Val Loss: 0.0148\n",
      "Epoch [35/100], Train Loss: 0.0071, Val Loss: 0.0153\n",
      "Epoch [40/100], Train Loss: 0.0061, Val Loss: 0.0149\n",
      "Epoch [45/100], Train Loss: 0.0059, Val Loss: 0.0139\n",
      "Epoch [50/100], Train Loss: 0.0056, Val Loss: 0.0142\n",
      "Epoch [55/100], Train Loss: 0.0054, Val Loss: 0.0142\n",
      "Epoch [60/100], Train Loss: 0.0045, Val Loss: 0.0146\n",
      "Epoch [65/100], Train Loss: 0.0038, Val Loss: 0.0139\n",
      "Epoch [70/100], Train Loss: 0.0043, Val Loss: 0.0147\n",
      "Epoch [75/100], Train Loss: 0.0043, Val Loss: 0.0143\n",
      "Epoch [80/100], Train Loss: 0.0031, Val Loss: 0.0144\n",
      "Epoch [85/100], Train Loss: 0.0037, Val Loss: 0.0136\n",
      "Epoch [90/100], Train Loss: 0.0035, Val Loss: 0.0137\n",
      "Epoch [95/100], Train Loss: 0.0033, Val Loss: 0.0125\n",
      "Epoch [100/100], Train Loss: 0.0034, Val Loss: 0.0134\n",
      "Best validation MSE: 0.0125\n",
      "Best validation RMSE: 0.11173226528074899\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class FundingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class FundingNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FundingNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = FundingDataset(X_train, y_train)\n",
    "val_dataset = FundingDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Setup training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FundingNet(input_size=X.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            val_loss += criterion(outputs, y_batch).item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict().copy()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model)\n",
    "print(f\"Best validation MSE: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation RMSE: {math.sqrt(best_val_loss):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/deepfunding/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.1278, Val Loss: 0.1282\n",
      "Epoch [10/100], Train Loss: 0.1271, Val Loss: 0.1455\n",
      "Epoch [15/100], Train Loss: 0.1272, Val Loss: 0.1270\n",
      "Epoch [20/100], Train Loss: 0.1259, Val Loss: 0.1261\n",
      "Early stopping triggered at epoch 23\n",
      "Best validation MSE: 0.1258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FundingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_size, d_model=128, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        \n",
    "        # Project input to d_model dimensions\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding is not needed since our input is not sequential\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Project input to d_model dimensions and add batch dimension for transformer\n",
    "        x = self.input_projection(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        \n",
    "        # Pass through transformer\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Take the output of the first (and only) token\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.regression_head(x)\n",
    "        return x\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = FundingDataset(X_train, y_train)\n",
    "val_dataset = FundingDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Setup training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerRegressor(\n",
    "    input_size=X.shape[1],\n",
    "    d_model=1576,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            val_loss += criterion(outputs, y_batch).item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model)\n",
    "print(f\"Best validation MSE: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
