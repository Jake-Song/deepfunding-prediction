{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from typing import Dict, List\n",
    "import polars as pl\n",
    "import httpx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_url = (\n",
    "    \"https://raw.githubusercontent.com/deepfunding/mini-contest/refs/heads/main/\"\n",
    ")\n",
    "\n",
    "df_train = pl.read_csv(f\"{repository_url}/dataset.csv\")\n",
    "df_test = pl.read_csv(f\"{repository_url}/test.csv\")\n",
    "\n",
    "# Light preprocessing to get project IDs instead of full URLs\n",
    "df_train = df_train.with_columns(\n",
    "    pl.col(\"project_a\").str.split(\"github.com/\").list.last().alias(\"project_a\"),\n",
    "    pl.col(\"project_b\").str.split(\"github.com/\").list.last().alias(\"project_b\"),\n",
    ")\n",
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"project_a\").str.split(\"github.com/\").list.last().alias(\"project_a\"),\n",
    "    pl.col(\"project_b\").str.split(\"github.com/\").list.last().alias(\"project_b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pl.concat(\n",
    "    [\n",
    "        df_train,\n",
    "        df_train.select(\n",
    "            \"id\",\n",
    "            pl.col(\"project_b\").alias(\"project_a\"),\n",
    "            pl.col(\"project_a\").alias(\"project_b\"),\n",
    "            pl.col(\"weight_b\").alias(\"weight_a\"),\n",
    "            pl.col(\"weight_a\").alias(\"weight_b\"),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repository_info(repository_id: str, client: httpx.Client) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetch repository information from GitHub API for a given repo URL.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.github.com/repos/{repository_id}\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"Authorization\": f\"Bearer {os.getenv('GITHUB_TOKEN')}\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.get(api_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except httpx.HTTPError:\n",
    "        print(f\"Error fetching data for {repository_id}\")\n",
    "        print(response.text)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projects_info(projects: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch project information from GitHub API for a list of project IDs and return as a Polars DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with httpx.Client(\n",
    "        transport=httpx.HTTPTransport(retries=5, verify=False),\n",
    "        follow_redirects=True,\n",
    "        limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),\n",
    "    ) as client:\n",
    "        for project_id in projects:\n",
    "            info = get_repository_info(project_id, client)\n",
    "            if info:\n",
    "                data.append(info)\n",
    "\n",
    "    df = pl.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            df_train.get_column(\"project_a\"),\n",
    "            df_train.get_column(\"project_b\"),\n",
    "            df_test.get_column(\"project_a\"),\n",
    "            df_test.get_column(\"project_b\"),\n",
    "        ]\n",
    "    )\n",
    "    .unique()\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects = get_projects_info(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_github_projects_data(\n",
    "    df: pl.DataFrame, df_projects: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add GitHub projects data to both projects in the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    df_projects = df_projects.select(\n",
    "        pl.col(\"full_name\").str.to_lowercase().alias(\"project_id\"),\n",
    "        pl.col(\"created_at\"),\n",
    "        pl.col(\"updated_at\"),\n",
    "        pl.col(\"size\"),\n",
    "        pl.col(\"stargazers_count\").alias(\"stars\"),\n",
    "        pl.col(\"watchers_count\").alias(\"watchers\"),\n",
    "        pl.col(\"forks_count\").alias(\"forks\"),\n",
    "        pl.col(\"open_issues_count\").alias(\"open_issues\"),\n",
    "        pl.col(\"subscribers_count\"),\n",
    "    )\n",
    "\n",
    "    df = df.join(\n",
    "        df_projects,\n",
    "        left_on=\"project_a\",\n",
    "        right_on=\"project_id\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_a\",\n",
    "    )\n",
    "\n",
    "    df = df.join(\n",
    "        df_projects,\n",
    "        left_on=\"project_b\",\n",
    "        right_on=\"project_id\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_b\",\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ratio_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract ratio-based features from repository data.\n",
    "    \"\"\"\n",
    "    features = df.clone()\n",
    "\n",
    "    # Basic ratios\n",
    "    features = features.with_columns(\n",
    "        [\n",
    "            (pl.col(\"stars\") / (pl.col(\"stars\") + pl.col(\"stars_b\"))).alias(\n",
    "                \"stars_ratio\"\n",
    "            ),\n",
    "            (pl.col(\"watchers\") / (pl.col(\"watchers\") + pl.col(\"watchers_b\"))).alias(\n",
    "                \"watchers_ratio\"\n",
    "            ),\n",
    "            (pl.col(\"forks\") / (pl.col(\"forks\") + pl.col(\"forks_b\"))).alias(\n",
    "                \"forks_ratio\"\n",
    "            ),\n",
    "            (pl.col(\"size\") / (pl.col(\"size\") + pl.col(\"size_b\"))).alias(\"size_ratio\"),\n",
    "            (\n",
    "                pl.col(\"open_issues\")\n",
    "                / (pl.col(\"open_issues\") + pl.col(\"open_issues_b\"))\n",
    "            ).alias(\"issues_ratio\"),\n",
    "            (\n",
    "                pl.col(\"subscribers_count\")\n",
    "                / (pl.col(\"subscribers_count\") + pl.col(\"subscribers_count_b\"))\n",
    "            ).alias(\"subscribers_count_ratio\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def extract_temporal_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract temporal features from repository data.\n",
    "    \"\"\"\n",
    "    features = df.clone()\n",
    "\n",
    "    if \"created_at\" in features.columns and \"updated_at\" in features.columns:\n",
    "        features = features.with_columns(\n",
    "            [\n",
    "                pl.col(\"created_at\")\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                .alias(\"created_dt\"),\n",
    "                pl.col(\"updated_at\")\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                .alias(\"updated_dt\"),\n",
    "                pl.col(\"created_at_b\")\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                .alias(\"created_dt_b\"),\n",
    "                pl.col(\"updated_at_b\")\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                .alias(\"updated_dt_b\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Calculate days since last update\n",
    "        now = pl.lit(datetime.now())\n",
    "        features = features.with_columns(\n",
    "            [\n",
    "                ((now - pl.col(\"updated_dt\")).dt.total_days()).alias(\n",
    "                    \"days_since_update\"\n",
    "                ),\n",
    "                ((now - pl.col(\"updated_dt_b\")).dt.total_days()).alias(\n",
    "                    \"days_since_update_b\"\n",
    "                ),\n",
    "                (\n",
    "                    (\n",
    "                        pl.col(\"updated_dt\").cast(pl.Int64)\n",
    "                        - pl.col(\"created_dt\").cast(pl.Int64)\n",
    "                    )\n",
    "                    / (24 * 3600)\n",
    "                ).alias(\"age_days\"),\n",
    "                (\n",
    "                    (\n",
    "                        pl.col(\"updated_dt_b\").cast(pl.Int64)\n",
    "                        - pl.col(\"created_dt_b\").cast(pl.Int64)\n",
    "                    )\n",
    "                    / (24 * 3600)\n",
    "                ).alias(\"age_days_b\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_v_index(df_dependent: pl.DataFrame, df_repo: pl.DataFrame) -> Dict[str, int]:\n",
    "  \"\"\"\n",
    "  Calculate V-Index of a software package.\n",
    "  V-Index is N where N is the number of first-order dependencies that have\n",
    "  at least N second-order dependencies.\n",
    "  \"\"\"\n",
    "  data = {}\n",
    "  for i in range(len(df_repo)):\n",
    "    repo_url = df_repo['repo_url'][i]\n",
    "    dependents = df_repo['list_of_dependents_in_oso'][i]\n",
    "\n",
    "    data[repo_url] = 0\n",
    "\n",
    "    # Filter the DataFrame for rows where 'package_artifact_name' is in the 'dependents' list\n",
    "    df_dependents = df_dependent.filter(df_dependent['package_artifact_name'].is_in(dependents))\n",
    "\n",
    "    # Count the number of rows in the filtered DataFrame\n",
    "    first_order_counts = df_dependents.height  # Alternatively, df_dependents.shape[0]\n",
    "\n",
    "    # Get the list of values in the 'num_dependents' column\n",
    "    second_order_counts = df_dependents['num_dependents'].to_list()\n",
    "\n",
    "    for j in range(first_order_counts):\n",
    "      if j + 1 > second_order_counts[j]:\n",
    "        data[repo_url] = j\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependent = pl.read_csv(\"data/dependent-metrics.csv\")\n",
    "df_repo = pl.read_parquet(\"data/repo_metrics_and_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_v_index_features(df: pl.DataFrame, df_v_index: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add v_index to the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.join(\n",
    "        df_v_index,\n",
    "        left_on=\"project_a\",\n",
    "        right_on=\"repo_url\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_a\",\n",
    "    )\n",
    "    df = df.join(\n",
    "        df_v_index,\n",
    "        left_on=\"project_b\",\n",
    "        right_on=\"repo_url\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_b\",\n",
    "    )\n",
    "    \n",
    "    eps = 1e-6\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"v_index\") / (pl.col(\"v_index\") + pl.col(\"v_index_b\") + eps)).alias(\"v_index_ratio\"),\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"stars\") * (pl.col(\"v_index\") + eps)).alias(\"stars_intersection_v_index\"),\n",
    "        (pl.col(\"stars_b\") * (pl.col(\"v_index_b\") + eps)).alias(\"stars_b_intersection_v_index_b\"),\n",
    "        (pl.col(\"stars_ratio\") * (pl.col(\"v_index_ratio\") + eps)).alias(\"stars_ratio_intersection_v_index_ratio\"),\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = add_github_projects_data(df_train, df_projects)\n",
    "df_train_full = extract_ratio_features(df_train_full)\n",
    "df_train_full = extract_temporal_features(df_train_full)\n",
    "\n",
    "df_test_full = add_github_projects_data(df_test, df_projects)\n",
    "df_test_full = extract_ratio_features(df_test_full)\n",
    "df_test_full = extract_temporal_features(df_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_index = calculate_v_index(df_dependent, df_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_index = pl.DataFrame({\"repo_url\": list(v_index.keys()), \"v_index\": list(v_index.values())})\n",
    "df_v_index = df_v_index.with_columns(\n",
    "        pl.col(\"repo_url\").str.split(\"github.com/\").list.last().alias(\"repo_url\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = add_v_index_features(df_train_full, df_v_index)\n",
    "df_test_full = add_v_index_features(df_test_full, df_v_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4774"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "   \"age_days\", \n",
    "   \"age_days_b\", \n",
    "   \"days_since_update\", \n",
    "   \"days_since_update_b\",    \n",
    "   \"stars\", \n",
    "   \"stars_b\", \n",
    "   \"stars_ratio\", \n",
    "   \"forks\", \n",
    "   \"forks_b\", \n",
    "   \"forks_ratio\", \n",
    "   \"open_issues\", \n",
    "   \"open_issues_b\", \n",
    "   \"issues_ratio\",\n",
    "   \"size\", \n",
    "   \"size_b\", \n",
    "   \"size_ratio\", \n",
    "   \"subscribers_count\", \n",
    "   \"subscribers_count_b\",  \n",
    "   \"subscribers_count_ratio\",\n",
    "   \"v_index\", \n",
    "   \"v_index_b\", \n",
    "   \"v_index_ratio\", \n",
    "   \"stars_intersection_v_index\",\n",
    "   \"stars_b_intersection_v_index_b\",\n",
    "   \"stars_ratio_intersection_v_index_ratio\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_full.select(features).to_numpy()\n",
    "\n",
    "y = df_train_full.get_column(\"weight_a\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 3819, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.495267\n",
      "[LightGBM] [Info] Total Bins 3469\n",
      "[LightGBM] [Info] Number of data points in the train set: 3819, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.503440\n",
      "[LightGBM] [Info] Total Bins 3469\n",
      "[LightGBM] [Info] Number of data points in the train set: 3819, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.494855\n",
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 3819, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.505230\n",
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 3820, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.501208\n",
      "Cross-validation MSE: 0.0191 (+/- 0.0011)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lgb_train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"mse\",\n",
    "    \"force_col_wise\": True,\n",
    "    \"num_leaves\": 100,\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Create training and validation datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.train(params, train_data, valid_sets=[val_data])\n",
    "\n",
    "    # Make predictions and calculate MSE\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = np.mean((y_val - y_pred) ** 2)\n",
    "    cv_scores.append(mse)\n",
    "\n",
    "# Calculate mean and std of MSE scores\n",
    "cv_scores = np.array(cv_scores)\n",
    "mean_mse = cv_scores.mean()\n",
    "std_mse = cv_scores.std()\n",
    "\n",
    "print(f\"Cross-validation MSE: {mean_mse:.4f} (+/- {std_mse:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 4774, number of used features: 25\n",
      "[LightGBM] [Info] Start training from score 0.500000\n"
     ]
    }
   ],
   "source": [
    "# Train model on the entire dataset\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_full.select(features).to_numpy()\n",
    "\n",
    "lgb_test_data = lgb.Dataset(X_test)\n",
    "\n",
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = pl.Series(test_predictions.tolist()).round(6).clip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-e61c9920aa444bef8f31046aa00800f2.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-e61c9920aa444bef8f31046aa00800f2.vega-embed details,\n",
       "  #altair-viz-e61c9920aa444bef8f31046aa00800f2.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-e61c9920aa444bef8f31046aa00800f2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e61c9920aa444bef8f31046aa00800f2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e61c9920aa444bef8f31046aa00800f2\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-205bebf20674f068f47e8ba1de00b4dd\"}, \"mark\": {\"type\": \"bar\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"importance\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"feature\", \"type\": \"nominal\"}}, \"params\": [{\"name\": \"param_2\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-205bebf20674f068f47e8ba1de00b4dd\": [{\"feature\": \"size_ratio\", \"importance\": 586}, {\"feature\": \"forks_ratio\", \"importance\": 577}, {\"feature\": \"stars_ratio\", \"importance\": 554}, {\"feature\": \"issues_ratio\", \"importance\": 546}, {\"feature\": \"age_days\", \"importance\": 532}, {\"feature\": \"age_days_b\", \"importance\": 525}, {\"feature\": \"size\", \"importance\": 485}, {\"feature\": \"size_b\", \"importance\": 470}, {\"feature\": \"subscribers_count_ratio\", \"importance\": 439}, {\"feature\": \"stars_b\", \"importance\": 422}, {\"feature\": \"stars\", \"importance\": 406}, {\"feature\": \"stars_ratio_intersection_v_index_ratio\", \"importance\": 402}, {\"feature\": \"v_index_ratio\", \"importance\": 397}, {\"feature\": \"open_issues\", \"importance\": 368}, {\"feature\": \"open_issues_b\", \"importance\": 368}, {\"feature\": \"forks\", \"importance\": 332}, {\"feature\": \"forks_b\", \"importance\": 324}, {\"feature\": \"stars_intersection_v_index\", \"importance\": 322}, {\"feature\": \"v_index_b\", \"importance\": 316}, {\"feature\": \"subscribers_count_b\", \"importance\": 295}, {\"feature\": \"v_index\", \"importance\": 294}, {\"feature\": \"stars_b_intersection_v_index_b\", \"importance\": 282}, {\"feature\": \"subscribers_count\", \"importance\": 258}, {\"feature\": \"days_since_update\", \"importance\": 202}, {\"feature\": \"days_since_update_b\", \"importance\": 198}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model.feature_importance()\n",
    "\n",
    "feature_importance = pl.DataFrame({\"feature\": features, \"importance\": importance.tolist()}).sort(\n",
    "    \"importance\", descending=True\n",
    ")\n",
    "\n",
    "feature_importance.plot.bar(x=\"importance\", y=\"feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x='weight_a', y='v_index_ratio', data=df_train_full)\n",
    "plt.title('Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df_test.select(pl.col(\"id\"), pl.Series(test_predictions).alias(\"pred\")).write_csv(\n",
    "    f\"data/submission_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-mse_{mean_mse:.6f}.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
